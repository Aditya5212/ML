{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9970ac3d",
   "metadata": {},
   "source": [
    "**user_message:**\n",
    "\n",
    "Focus on the following in the table: cell [7, 5] in sheet 'null'\n",
    "        A. To use PCA Algorithm for dimensionality reduction.\n",
    "You have a dataset that includes measurements for different variables on wine\n",
    "(alcohol, ash, magnesium, and so on). Apply PCA algorithm & transform this data\n",
    "so that most variations in the measurements of the variables are captured by a small\n",
    "number of principal components so that it is easier to distinguish between red and\n",
    "white wine by inspecting these principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926a9ff6",
   "metadata": {},
   "source": [
    "**ai_message:**\n",
    "\n",
    "Certainly! I'll focus on applying the PCA (Principal Component Analysis) algorithm for dimensionality reduction on the wine dataset. This will help us capture the most significant variations in the measurements using a smaller number of principal components. Let's get started by loading the data, preparing it for PCA, and then applying the algorithm.\n",
    "\n",
    "First, let's load the data and take a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ec7d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('Wine.csv')\n",
    "\n",
    "# Display the first few rows and basic information\n",
    "print(df.head())\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9eb173",
   "metadata": {},
   "source": [
    "**ai_message:**\n",
    "\n",
    "\n",
    "\n",
    "Now that we have loaded and inspected the data, let's proceed with applying PCA for dimensionality reduction. We'll standardize the features, apply PCA, and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20251387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop('Customer_Segment', axis=1)\n",
    "y = df['Customer_Segment']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Calculate the cumulative explained variance ratio\n",
    "cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Plot the cumulative explained variance ratio\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, 'bo-')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.title('Cumulative Explained Variance Ratio vs. Number of Components')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print the explained variance ratio for each component\n",
    "print(\"Explained variance ratio for each component:\")\n",
    "for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
    "    print(f\"PC{i+1}: {ratio:.4f}\")\n",
    "\n",
    "# Select the first two principal components for visualization\n",
    "X_pca_2d = X_pca[:, :2]\n",
    "\n",
    "# Create a scatter plot of the first two principal components\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=y, cmap='viridis')\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.title('Wine Data - First Two Principal Components')\n",
    "plt.colorbar(scatter, label='Customer Segment')\n",
    "plt.show()\n",
    "\n",
    "# Print the feature importance (loadings) for the first two principal components\n",
    "feature_names = X.columns\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_.T[:, :2],\n",
    "    columns=['PC1', 'PC2'],\n",
    "    index=feature_names\n",
    ")\n",
    "print(\"\\nFeature loadings for the first two principal components:\")\n",
    "print(loadings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac60ac39",
   "metadata": {},
   "source": [
    "**ai_message:**\n",
    "\n",
    "\n",
    "\n",
    "Based on the PCA analysis of the wine dataset, here are the key findings:\n",
    "\n",
    "1. Cumulative Explained Variance:\n",
    "{image_urls_dict['c40f9b92']}\n",
    "\n",
    "This plot shows how much of the total variance in the data is explained by each principal component. We can see that:\n",
    "- The first two principal components (PC1 and PC2) together explain about 55% of the total variance.\n",
    "- To capture 80% of the variance, we would need about 5-6 principal components.\n",
    "\n",
    "2. Explained Variance Ratio:\n",
    "The explained variance ratio for each component shows how much of the total variance is explained by each principal component:\n",
    "\n",
    "{outputs_dict['3e835e4f']}\n",
    "{outputs_dict['a63bbd8d']}\n",
    "{outputs_dict['0364d90d']}\n",
    "{outputs_dict['944ef980']}\n",
    "{outputs_dict['d77e901d']}\n",
    "\n",
    "PC1 explains 36.20% of the variance, while PC2 explains 19.21%. Together, they account for 55.41% of the total variance in the data.\n",
    "\n",
    "3. Visualization of First Two Principal Components:\n",
    "{image_urls_dict['f29d2cd4']}\n",
    "\n",
    "This scatter plot shows the wine samples projected onto the first two principal components. Each point represents a wine sample, and the color represents the customer segment (which likely corresponds to different types of wine). We can observe some separation between the clusters, indicating that these two components can help distinguish between different wine types to some extent.\n",
    "\n",
    "4. Feature Loadings:\n",
    "{outputs_dict['bfd7847e']}\n",
    "{outputs_dict['e939f14a']}\n",
    "\n",
    "These loadings show how much each original feature contributes to PC1 and PC2. Some observations:\n",
    "\n",
    "- PC1 is most strongly influenced by Total_Phenols (positive), Malic_Acid (negative), and Ash_Alcanity (negative).\n",
    "- PC2 is most strongly influenced by Alcohol (negative), Ash (negative), and Magnesium (negative).\n",
    "\n",
    "In conclusion, the PCA analysis has successfully reduced the dimensionality of the wine dataset. The first two principal components capture more than half of the total variance in the data and show some ability to distinguish between different wine types. To capture more of the variance, you might consider using 3-5 principal components, depending on your specific needs for dimensionality reduction and information retention.\n",
    "\n",
    "This analysis provides a good starting point for further exploration and modeling of the wine dataset, allowing you to focus on the most important features that contribute to the differences between wine types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adab22bb",
   "metadata": {},
   "source": [
    "**user_message:**\n",
    "\n",
    "can you Do same task without using PCA default function insted of by using step wise representation of eigen vector and eigen value also giving me the working of each line of code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17074d33",
   "metadata": {},
   "source": [
    "**ai_message:**\n",
    "\n",
    "Certainly! I'll implement PCA step-by-step without using the default PCA function. We'll calculate the eigenvectors and eigenvalues manually. I'll provide a detailed explanation of each step in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de92815f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Load and prepare the data\n",
    "df = pd.read_csv('Wine.csv')\n",
    "X = df.drop('Customer_Segment', axis=1)\n",
    "y = df['Customer_Segment']\n",
    "\n",
    "# Step 2: Standardize the features\n",
    "def standardize(X):\n",
    "    return (X - X.mean()) / X.std()\n",
    "\n",
    "X_std = standardize(X)\n",
    "\n",
    "# Step 3: Calculate the covariance matrix\n",
    "def calc_covariance_matrix(X):\n",
    "    return np.cov(X.T)\n",
    "\n",
    "cov_matrix = calc_covariance_matrix(X_std)\n",
    "\n",
    "# Step 4: Calculate eigenvectors and eigenvalues\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# Step 5: Sort eigenvectors by decreasing eigenvalues\n",
    "idx = eigenvalues.argsort()[::-1]\n",
    "eigenvalues = eigenvalues[idx]\n",
    "eigenvectors = eigenvectors[:, idx]\n",
    "\n",
    "# Step 6: Calculate explained variance ratio\n",
    "total_variance = np.sum(eigenvalues)\n",
    "explained_variance_ratio = eigenvalues / total_variance\n",
    "\n",
    "# Step 7: Project data onto principal components\n",
    "def project_data(X, eigenvectors, n_components):\n",
    "    return np.dot(X, eigenvectors[:, :n_components])\n",
    "\n",
    "X_pca = project_data(X_std, eigenvectors, 2)\n",
    "\n",
    "# Print explained variance ratio for each component\n",
    "print(\"Explained variance ratio for each component:\")\n",
    "for i, ratio in enumerate(explained_variance_ratio):\n",
    "    print(f\"PC{i+1}: {ratio:.4f}\")\n",
    "\n",
    "# Plot cumulative explained variance ratio\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(explained_variance_ratio) + 1), np.cumsum(explained_variance_ratio), 'bo-')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.title('Cumulative Explained Variance Ratio vs. Number of Components')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot first two principal components\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.title('Wine Data - First Two Principal Components')\n",
    "plt.colorbar(scatter, label='Customer Segment')\n",
    "plt.show()\n",
    "\n",
    "# Print feature loadings for the first two principal components\n",
    "feature_names = X.columns\n",
    "loadings = pd.DataFrame(\n",
    "    eigenvectors[:, :2],\n",
    "    columns=['PC1', 'PC2'],\n",
    "    index=feature_names\n",
    ")\n",
    "print(\"\\nFeature loadings for the first two principal components:\")\n",
    "print(loadings)\n",
    "\n",
    "# Explanation of each step\n",
    "print(\"\\nExplanation of each step:\")\n",
    "print(\"1. Load and prepare data: We load the Wine.csv file and separate features (X) from the target variable (y).\")\n",
    "print(\"2. Standardize features: We subtract the mean and divide by the standard deviation for each feature.\")\n",
    "print(\"3. Calculate covariance matrix: We compute the covariance matrix of the standardized features.\")\n",
    "print(\"4. Calculate eigenvectors and eigenvalues: We use numpy's linalg.eig function to compute eigenvectors and eigenvalues of the covariance matrix.\")\n",
    "print(\"5. Sort eigenvectors: We sort eigenvectors in descending order of their corresponding eigenvalues.\")\n",
    "print(\"6. Calculate explained variance ratio: We divide each eigenvalue by the sum of all eigenvalues to get the proportion of variance explained by each principal component.\")\n",
    "print(\"7. Project data: We multiply the standardized data by the sorted eigenvectors to get the principal components.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50479f66",
   "metadata": {},
   "source": [
    "**ai_message:**\n",
    "\n",
    "Here is the step-by-step implementation of PCA without using the default PCA function, along with explanations for each step:\n",
    "\n",
    "1. **Load and Prepare Data**: We load the `Wine.csv` file and separate features (X) from the target variable (y).\n",
    "\n",
    "2. **Standardize Features**: We subtract the mean and divide by the standard deviation for each feature to standardize the data.\n",
    "\n",
    "3. **Calculate Covariance Matrix**: We compute the covariance matrix of the standardized features.\n",
    "\n",
    "4. **Calculate Eigenvectors and Eigenvalues**: We use NumPy's `linalg.eig` function to compute eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "5. **Sort Eigenvectors**: We sort eigenvectors in descending order of their corresponding eigenvalues.\n",
    "\n",
    "6. **Calculate Explained Variance Ratio**: We divide each eigenvalue by the sum of all eigenvalues to get the proportion of variance explained by each principal component.\n",
    "\n",
    "7. **Project Data**: We multiply the standardized data by the sorted eigenvectors to get the principal components.\n",
    "\n",
    "### Results:\n",
    "\n",
    "- **Explained Variance Ratio for Each Component**:\n",
    "  {outputs_dict['df71f3c7']}\n",
    "  {outputs_dict['62c5af74']}\n",
    "  {outputs_dict['5fba2f1d']}\n",
    "  {outputs_dict['382d6968']}\n",
    "  {outputs_dict['de52b998']}\n",
    "\n",
    "- **Feature Loadings for the First Two Principal Components**:\n",
    "  {outputs_dict['2e6ff19a']}\n",
    "  {outputs_dict['33039b71']}\n",
    "\n",
    "- **Cumulative Explained Variance Ratio Plot**:\n",
    "  {image_urls_dict['d9c7347c']}\n",
    "\n",
    "- **First Two Principal Components Plot**:\n",
    "  {image_urls_dict['2b367bae']}\n",
    "\n",
    "### Explanation of Each Step:\n",
    "{outputs_dict['ed2bb39b']}\n",
    "{outputs_dict['cb4bdd09']}\n",
    "{outputs_dict['77da26a6']}\n",
    "{outputs_dict['53645642']}\n",
    "{outputs_dict['e29d70c8']}\n",
    "{outputs_dict['c766546e']}\n",
    "{outputs_dict['4ec293f4']}\n",
    "{outputs_dict['3dc9ccb8']}\n",
    "\n",
    "This step-by-step approach allows us to understand the inner workings of PCA, from standardizing the data to projecting it onto the principal components."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
